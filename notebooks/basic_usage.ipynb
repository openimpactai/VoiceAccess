{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VoiceAccess Basic Usage Tutorial\n",
    "\n",
    "This notebook demonstrates basic usage of VoiceAccess for speech recognition in low-resource languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from src.core.asr_engine import ASREngine\n",
    "from src.core.config import Config\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize ASR Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load default configuration\n",
    "config = Config()\n",
    "\n",
    "# Or load from file\n",
    "# config = Config.from_file(\"../configs/default.yaml\")\n",
    "\n",
    "# Initialize ASR engine\n",
    "engine = ASREngine(config)\n",
    "print(f\"ASR Engine initialized with device: {config.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model\n",
    "# Note: You need to download or train a model first\n",
    "model_path = \"../models/pretrained/wav2vec2-base.pt\"\n",
    "model_type = \"wav2vec2\"\n",
    "\n",
    "try:\n",
    "    engine.load_model(model_path, model_type=model_type)\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    print(f\"Model size: {engine.model.get_model_size_mb():.2f} MB\")\n",
    "    print(f\"Parameters: {engine.model.get_num_params():,}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Model file not found at {model_path}\")\n",
    "    print(\"Please download a pre-trained model or train your own.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transcribe Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Transcribe a single audio file\n",
    "audio_path = \"../data/raw/example.wav\"\n",
    "\n",
    "# Display audio\n",
    "try:\n",
    "    ipd.display(ipd.Audio(audio_path))\n",
    "    \n",
    "    # Transcribe\n",
    "    transcription = engine.transcribe(audio_path)\n",
    "    print(f\"\\nTranscription: {transcription}\")\n",
    "    \n",
    "    # With confidence score\n",
    "    text, confidence = engine.transcribe(audio_path, return_confidence=True)\n",
    "    print(f\"\\nTranscription: {text}\")\n",
    "    print(f\"Confidence: {confidence:.2%}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Audio file not found at {audio_path}\")\n",
    "    print(\"Please add audio files to the data directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe multiple audio files\n",
    "audio_files = [\n",
    "    \"../data/raw/audio1.wav\",\n",
    "    \"../data/raw/audio2.wav\",\n",
    "    \"../data/raw/audio3.wav\"\n",
    "]\n",
    "\n",
    "# Filter existing files\n",
    "existing_files = [f for f in audio_files if Path(f).exists()]\n",
    "\n",
    "if existing_files:\n",
    "    transcriptions = engine.transcribe_batch(existing_files)\n",
    "    \n",
    "    for audio_file, transcription in zip(existing_files, transcriptions):\n",
    "        print(f\"\\nFile: {Path(audio_file).name}\")\n",
    "        print(f\"Transcription: {transcription}\")\n",
    "else:\n",
    "    print(\"No audio files found for batch transcription.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Language Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt model to a new language\n",
    "# This requires adaptation data in the specified format\n",
    "\n",
    "language_code = \"xyz\"  # Replace with your language code\n",
    "adaptation_data_path = f\"../data/{language_code}/\"\n",
    "\n",
    "if Path(adaptation_data_path).exists():\n",
    "    print(f\"Adapting model to language: {language_code}\")\n",
    "    \n",
    "    # Perform adaptation\n",
    "    engine.adapt_to_language(language_code, adaptation_data_path)\n",
    "    \n",
    "    # Save adapted model\n",
    "    adapted_model_path = f\"../models/finetuned/{model_type}-{language_code}.pt\"\n",
    "    engine.model.save_checkpoint(adapted_model_path)\n",
    "    print(f\"Adapted model saved to: {adapted_model_path}\")\n",
    "else:\n",
    "    print(f\"No adaptation data found at {adaptation_data_path}\")\n",
    "    print(\"Please prepare your language data first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "test_data_path = \"../data/test/\"\n",
    "\n",
    "if Path(test_data_path).exists():\n",
    "    print(\"Evaluating model performance...\")\n",
    "    \n",
    "    # Evaluate with WER and CER metrics\n",
    "    results = engine.evaluate(\n",
    "        test_data_path,\n",
    "        metrics=[\"wer\", \"cer\"]\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for metric, value in results.items():\n",
    "        print(f\"{metric.upper()}: {value:.2%}\")\n",
    "else:\n",
    "    print(f\"No test data found at {test_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate audio preprocessing\n",
    "from src.preprocessing.audio_processor import AudioProcessor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create audio processor\n",
    "processor = AudioProcessor(config)\n",
    "\n",
    "# Generate example audio\n",
    "duration = 3  # seconds\n",
    "sample_rate = config.sample_rate\n",
    "t = np.linspace(0, duration, duration * sample_rate)\n",
    "frequency = 440  # A4 note\n",
    "waveform = 0.5 * np.sin(2 * np.pi * frequency * t)\n",
    "\n",
    "# Add some noise\n",
    "noise = 0.05 * np.random.randn(len(waveform))\n",
    "waveform = waveform + noise\n",
    "\n",
    "# Process audio\n",
    "features = processor.process(waveform)\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n",
    "\n",
    "# Plot waveform\n",
    "ax1.plot(t[:1000], waveform[:1000])\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Amplitude')\n",
    "ax1.set_title('Audio Waveform (first 1000 samples)')\n",
    "\n",
    "# Plot features\n",
    "if features.ndim > 1:\n",
    "    ax2.imshow(features.T, aspect='auto', origin='lower')\n",
    "    ax2.set_xlabel('Time frames')\n",
    "    ax2.set_ylabel('Feature dimension')\n",
    "    ax2.set_title('Processed Features')\n",
    "else:\n",
    "    ax2.plot(features[:1000])\n",
    "    ax2.set_xlabel('Sample')\n",
    "    ax2.set_ylabel('Amplitude')\n",
    "    ax2.set_title('Processed Waveform (first 1000 samples)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original waveform shape: {waveform.shape}\")\n",
    "print(f\"Processed features shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configuration Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show current configuration\n",
    "print(\"Current Configuration:\")\n",
    "print(f\"Model Type: {config.model_type}\")\n",
    "print(f\"Sample Rate: {config.sample_rate} Hz\")\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Batch Size: {config.batch_size}\")\n",
    "print(f\"Learning Rate: {config.learning_rate}\")\n",
    "\n",
    "# Update configuration\n",
    "config.update(\n",
    "    batch_size=16,\n",
    "    learning_rate=5e-5\n",
    ")\n",
    "\n",
    "print(\"\\nUpdated Configuration:\")\n",
    "print(f\"Batch Size: {config.batch_size}\")\n",
    "print(f\"Learning Rate: {config.learning_rate}\")\n",
    "\n",
    "# Save configuration\n",
    "config.to_file(\"../configs/custom_config.yaml\")\n",
    "print(\"\\nConfiguration saved to: configs/custom_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Prepare Your Data**: Organize your audio files and transcriptions\n",
    "2. **Train Models**: Use the training scripts to train on your language\n",
    "3. **Fine-tune**: Adapt existing models to your specific use case\n",
    "4. **Deploy**: Use the API server for production deployment\n",
    "5. **Contribute**: Share your models and improvements with the community\n",
    "\n",
    "For more examples, check the `examples/` directory and our documentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}